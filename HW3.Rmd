---
title: "HW#3"
author: "Halid Kopanski"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
library(faraway)
library(car)
library(MASS)
library(lmtest)
library(tidyverse)
library(nlme)
```

## LMR 4.1

```{r echo = FALSE}
pros_model1 <- lm(lpsa ~ ., data = prostate)
summary(pros_model1)
```

a)

```{r echo=FALSE}
new_patient <- tibble(lcavol = 1.44692,  lweight = 3.62301,  age = 65.00000, 
                      lbph = 0.30010,  svi = 0.00000,  lcp = -0.79851, 
                      gleason = 7.0000,   pgg45 = 15.0000)

predict(pros_model1, newdata = new_patient, interval = "confidence")
#predict(pros_model1, newdata = new_patient, interval = "prediction")
mean(prostate$age)
```

b)

```{r echo = FALSE}
new_patient1 <- tibble(lcavol = 1.44692,  lweight = 3.62301,  age = 25.00000, 
                      lbph = 0.30010,  svi = 0.00000,  lcp = -0.79851, 
                      gleason = 7.0000,   pgg45 = 15.0000)

print(new_patient1)
predict(pros_model1, newdata = new_patient1, interval = "confidence")
#predict(pros_model1, newdata = new_patient1, interval = "prediction")
```

In the case of the 65 year old, their data fit within the original data. The mean age of the data set was `r round(mean(prostate$age), 2)` which is quite close to the patient's age. The 20 year was too far from the mean of the original data and there fore introduced higher variance.

c)

```{r echo = FALSE}
pros_model2 <- lm(lpsa ~ lcavol + lweight + svi, data = prostate)
summary(pros_model2)
predict(pros_model2, newdata = new_patient1, interval = "confidence")
predict(pros_model2, newdata = new_patient1, interval = "prediction")
print(anova(pros_model2, pros_model1))
```

There is less error in the reduced model due to the less degrees of freedom. This reduced the uncertainty in the prediction. The reduced model is preferable due to less error and less likely to be overfit to the original data. 

\newpage

## LMR 6.1

```{r echo = FALSE}
sat_model1 <- lm(total ~ expend + salary + ratio + takers, data = sat)
summary(sat_model1)
```

a)

```{r echo = FALSE}
plot(x = sat_model1$fitted.values, y = sat_model1$residuals, pch = 19,
     main = "Residuals vs Fitted Values", xlab = "Fitted Values", ylab = "Residuals")
res_line <- loess(sat_model1$residuals ~ sat_model1$fitted.values)
j <- order(sat_model1$fitted.values)
lines(sat_model1$fitted.values[j], res_line$fitted[j], col = "red", lwd = 3)
abline(h = 0)
```

Results of the Breusch - Pagan Test 

The plot seems to show that there is a slight curve to the residuals which would point to the data variance not being constant but the Breusch Pagan test results indicate otherwise. The test did not provide sufficient evidence to reject the null hypothesis which states that the data variance is constant (alternate hypothesis states it is not constant).

```{r echo = FALSE}
bptest(sat_model1, studentize = FALSE)
```

The results of the Non Constant Variance Test

```{r echo = FALSE}
ncvTest(sat_model1)
```

b)

```{r echo = FALSE}
qqnorm(sat_model1$residuals, ylab = "Residuals", main = "Q-Q Plot (Normality)", pch = 19)
qqline(sat_model1$residuals)
```

Results of the Shapiro and the Durbin-Watson Tests

```{r echo = FALSE}
shapiro.test(sat_model1$residuals)
dwtest(sat_model1)
```

The QQ plot does indicate some deviation from normality at higher quantiles, but the shapiro test did not provide enough evidence to reject the null hypothesis of the data being normal.

c)

```{r echo = FALSE, fig.dim = c(6,6)}
X <- sat %>% select(expend, salary, ratio, takers)
X <- cbind(rep(1, nrow(X)), X)

X <- data.matrix(X)

#print(X)
Xt <- t(X)

XtX_inv <- solve(Xt %*% X)

XtY <- Xt %*% sat$total

beta_hat <- XtX_inv %*% XtY

P <- X %*% XtX_inv %*% Xt

res_mean <- mean(sat_model1$residuals)
res_sd <- sd(sat_model1$residuals)
stan_res <- (sat_model1$residuals - res_mean) / res_sd

plot(x = diag(P), 
     y = sat_model1$residuals, 
     pch = 19,
     main = "Residuals vs Leverage", 
     xlab = "Leverage", 
     ylab = "Residuals",
     xlim = c(0, 0.35))
text(diag(P), sat_model1$residuals, rownames(P), 
     cex = 0.6, pos = 4, col = "red")
```

```{r echo = FALSE}
hat_data <- cbind(1:50, diag(P))

plot(hat_data, pch = 19, xlab = "", ylab = "hii values")
abline(h = 2 * 5 / 50)
text(hat_data[,1], hat_data[,2], names(diag(P)), 
     cex = 0.5, pos = 4, col = "red")
```


```{r echo = FALSE, fig.dim = c(6,6)}
dotchart(sort(diag(P)), pch = 19, cex = 0.5)
```

A number of high leverage points exists. Mainly Utah and California but also Connecticut and New Jersey. Dropping Utah and California and refitting the model should be considered. Centering is also an option.

d)

```{r echo = FALSE}
outlierTest(sat_model1)

plot(x = sat_model1$fitted.values, y = rstandard(sat_model1), pch = 19,
     main = "Standardized Residuals vs Fitted Values", xlab = "Fitted Values", ylab = "Standardized Residuals")
abline(h = c(0, 2, -2), col = "blue", lty = 4, lwd = 2)
text(x = sat_model1$fitted.values, y = rstandard(sat_model1), rownames(sat), 
     cex = 0.5, pos = 4, col = "red")
```

```{r echo = FALSE}
#cooks.distance(sat_model1)

halfnorm(diag(P), labs = rownames(P), ylab = "Leverages")
abline(h = 2 * sat_model1$rank / nrow(sat))

SST <- sum((sat$total - mean(sat$total))^2)
SSR <- sum((sat_model1$fitted.values - mean(sat$total))^2)
SSE <- sum((sat$total - sat_model1$fitted.values)^2)
MSR <- SSR/(sat_model1$rank - 1)
MSE <- SSE/(nrow(sat) - sat_model1$rank)
```

The outlier test did not reveal any outliers, but some of the plots indicate that Utah and California do stand out. Dropping them should be considered.

e)

```{r echo = FALSE, fig.dim = c(6,6)}
influenceIndexPlot(sat_model1)
```

```{r echo = FALSE, fig.dim = c(6,6)}
cooks_dis_calc <- sat_model1$residuals^2 / 
  (sat_model1$rank * MSE) * (diag(P)/(1-diag(P))^2)

dotchart(sort(cooks_dis_calc), cex = 0.5, pch = 19)
```

Utah appears to be influential, in addition to having high leverage and being an outlier. Refitting the model without Utah would be recommended.

f)

```{r echo = FALSE}
pairs(sat)
```

Total is strongly linear with verbal and math, but this is not surprising. Takers indicates somewhat of a negative relationship with total. The remaining three appear not to have a strong relationship with total.

\newpage

## LMR 7.8

a)

```{r echo = FALSE}
fat_model1 <- lm(brozek ~ age + weight + height + neck + chest + 
                   abdom + hip + thigh + knee + ankle + biceps + 
                   forearm + wrist, data = fat)

X <- model.matrix(fat_model1)[,-1]
eigen_X <- eigen(t(X) %*% X)

sqrt(eigen_X$values[1]/eigen_X$values)

vif_x <- rep(0, ncol(X))


for(i in 1:ncol(X)){
    vif_x[i] <- 1 / (1 - summary(lm(X[,i] ~ X[,-i]))$r.squared)    
}

vif_x <- tibble(colnames(X), vif_x)
print(vif_x)
```

We can see some quite large condition and VIF numbers. Eigenvalues over 30 point to collinearity, of which there are a few. Inspecting the VIF table, shows a number of values over 10 (weight. abdom, hip, and maybe chest). Overall, it can be said that there is a high degree of collinearity in the data.  

b)

```{r echo = FALSE}
new_fat <- fat[-c(39,42),]

fat_model2 <- lm(brozek ~ age + weight + height + neck + chest + 
                   abdom + hip + thigh + knee + ankle + biceps + 
                   forearm + wrist, data = new_fat)

X2 <- model.matrix(fat_model2)[,-1]
eigen_X2 <- eigen(t(X2) %*% X2)

sqrt(eigen_X2$values[1]/eigen_X2$values)

vif_x2 <- rep(0, ncol(X2))


for(i in 1:ncol(X2)){
    vif_x2[i] <- 1 / (1 - summary(lm(X2[,i] ~ X2[,-i]))$r.squared)    
}

vif_x2 <- tibble(colnames(X2), vif_x2)
print(vif_x2)
```

Dropping the values did not improve collinearity in the model. It made is worse in some cases.

c)

```{r echo = FALSE}

fat_model3 <- lm(brozek ~ age + weight + height, data = new_fat)
summary(fat_model3)

X3 <- model.matrix(fat_model3)[,-1]
eigen_X3 <- eigen(t(X3) %*% X3)

sqrt(eigen_X3$values[1]/eigen_X3$values)

vif_x3 <- rep(0, ncol(X3))


for(i in 1:ncol(X3)){
    vif_x3[i] <- 1 / (1 - summary(lm(X3[,i] ~ X3[,-i]))$r.squared)    
}

vif_x3 <- cbind(colnames(X3), vif_x3)
print(vif_x3)

```

The reduced model significantly mitigated collinearity. All condition and VIF numbers are well within desired limits. This model is much better than the full model in terms of collinearity.

d)

```{r echo = FALSE}

fat_1 <- as.data.frame(t(apply(X3, 2, median)))

predict(fat_model3, newdata = fat_1, interval = "prediction")

```

e)

```{r echo = FALSE}

fat_2 <- as.data.frame(cbind(age = 40, weight = 200, height = 73))

predict(fat_model3, newdata = fat_2, interval = "prediction")

```

The two predictions both spanned roughly 20. This indicates the model is good for that range of values.

f)

```{r echo = FALSE}
fat_3 <- as.data.frame(cbind(age = 40, weight = 130, height = 73))

predict(fat_model3, newdata = fat_3, interval = "prediction")
```

Negative values are in the range which are impossible. The values for age, weight, and height are too far different than the original dataset. This introduces a lot of error into the prediction.

\newpage

## Question 4

### Part A
```{r echo = FALSE}
df_commercial <- read_table("commercial_property.txt")
#a)
options(repr.plot.width = 8, repr.plot.height = 6, repr.plot.res = 150)
df_commercial %>% pairs()
```

The only pair of variables that show a relationship is between tot_sqft and rental_rates which displays a slight linear relationship. The other pairs do not show strong relationship, linear or otherwise.

### Part B

```{r echo = FALSE}
#b)
cor(df_commercial)
```

The strongest positive correlation is the one between tot_sqft and rental_rates. The strongest negative is the one between opp_expenses and vac_rates. In both of those cases, the relationship was moderate. Age showed a moderate negative ralation with rental_rates and vac_rates. It also had a moderate positive relationship with tot_squft and opp_expenses. Overall, the only two pairs that did not indicate a significant relationship is vac_rates/rental_rates and vac_rates/tot_squft. 

### Part C
```{r echo = FALSE}
#c)
comm_model1 <- lm(rental_rates ~ ., data = df_commercial)
summary(comm_model1)
comm_model1$coefficients
```

**c. part a) through c)**  
y = 12.2 - 0.142 $x_1$ + 0.282 $x_2$ + 0.620 $x_3$ + 0.000000792 $x_4$  
where:  
$x_1$ -> age of dwelling (age)  
$x_2$ -> operating expenses and taxes (opp_expenses)  
$x_3$ -> vacancy rates (vac_rates)  
$x_4$ -> total square footage (tot_squft) 

The predictor with seemingly the smallest effect on rental rates is total square footage as can be seen by the near zero value, but it is still statistically significant. The small coefficient value is offset by the large values of the predictor itself. Vacancy rates were the least statistically significant predictor as can be seen with the low p value. If this predictor is dropped from the model, the model prediction will still produce a reasonably accurate result. Age and operating expenses were both statistically significant to the model and produced a large effect on the resulting prediction.

```{r echo = FALSE}
sprintf("The R squared values is %.3f", summary(comm_model1)$r.squared)
sprintf("The adjusted R squared values is %.3f", summary(comm_model1)$adj.r.squared)
```


**c. part d)**
The null hypothesis, $H_0$, states that all 4 coefficients are equal to zero. The alternate hypothesis, $H_A$ states that at least one of the coefficients is not zero. The test statistic is given as a the F-statistic in the model summary which is equal to 26.76. This results in a p value of nearly zero using an f distribution of degrees of freedom of 4 and 76. Since the p value is less than the 0.05 significance level, we have enough evidence to reject the null hypothesis in favor of the alternative hypothesis.

**c. part e)**
```{r echo = FALSE}
#c. part e)

summary(gls(rental_rates ~ ., data = df_commercial))
```

**c. part f)**

The test statistic for $\beta_3$ is 0.570 which gives a p value of 0.5704. If $H_0$ states that $\beta_3 = 0$ and $H_A$ states that $\beta_3$ is not equal to zero, then in this case we do no have enough evidence to reject the null hypothesis. This means that there is not enough evidence to reject the possibility that $\beta_3$ is zero. We can then say that $\beta$ is not statistically significant to the model.

**c. part h)**

```{r echo = FALSE}
# c. part h)

comm_model2 <- lm(rental_rates ~ .-vac_rates, data = df_commercial)
summary(comm_model2)
```

### Part D

**d. part I)**

```{r echo = FALSE}
#d. part I)
model_coeff <- summary(comm_model2)$coefficients

model_coeff <- as_tibble(model_coeff)

colnames(model_coeff)[2] = 'Error'

model_coeff <- model_coeff %>% mutate(CI90neg = Estimate - qt(0.95, 77) * Error, CI90pos = Estimate + qt(0.95, 77) * Error)

knitr::kable(model_coeff)
```

**d. part II**

```{r echo = FALSE}
#d. part II)

pvals <- summary(comm_model2)$coef[,4]
padj <- p.adjust(pvals, method="bonferroni")
print(coef(comm_model2)[padj < 0.1])
```

**d. part III**
```{r echo = FALSE}
#d. part III)
new_dwellings <- tibble(age = c(5.0, 6.0, 14.0, 12.0),
                        opp_expenses = c(8.25, 8.50, 11.50, 10.25),
                        vac_rates = rep(0, 4),
                        tot_squft = c(250000, 270000, 300000, 310000))

new_dwellings <- new_dwellings %>% mutate(pred_rrates = predict(comm_model2, newdata = new_dwellings))
print(new_dwellings)
```

**d. part IV**
```{r echo = FALSE}
#d part IV)
new_dwellings2 <- tibble(age = c(4.0, 6.0, 12.0),
                         opp_expenses = c(10.0, 11.50, 12.5),
                         vac_rates = rep(0, 3),
                         tot_squft = c(80000, 120000, 340000))
new_dwellings2 <- new_dwellings2 %>% mutate(pred_rrates = predict(comm_model2, newdata = new_dwellings2))
print(new_dwellings2)
predict(comm_model2, newdata = new_dwellings2, interval = "prediction", level = 0.95)
```

### Part E

**e. part a)**

```{r echo = FALSE}

plot(x = comm_model2$fitted.values, y = comm_model2$residuals, pch = 19,
     main = "Residuals vs Fitted Values", xlab = "Fitted Values", ylab = "Residuals")

```

The plot indicates that variance is constant and that the data is normal. A deeper investigation is needed in order to confirm the strength of these assessments.

**e. part b)**

```{r echo =FALSE}
# e part b)
qqnorm(comm_model2$residuals, ylab = "Residuals", main = "Q-Q Plot (Normality)", pch = 19)
qqline(comm_model2$residuals)
```

The QQ plot shows some deviation from normality. This means that the data was not strictly following a normal distribution. 

```{r echo = FALSE}
boxplot(comm_model2$residuals)
```

The boxplot shows a number of outliers at both ends of the distribution. These should be investigated more using specific outlier testing methods.

```{r echo = FALSE}
hist(comm_model2$residuals, breaks = 20, xlab = "Model Residuals", main = "Histogram of Residuals", prob = TRUE)
lines(density(comm_model2$residuals), col = "red") 
```

The histogram shows more evidence for outliers and even some skew in the data.

**e. part c)**
```{r echo = FALSE}
#e part c)
bptest(comm_model2, studentize = FALSE)
```

**e part d and g)**
```{r echo = FALSE}
#e part d and g
influenceIndexPlot(comm_model2)
```

The diagnostic plots do reveal a few outliers, high leverage, and influential points. Data points 6 and 62 are outliers, while 3, 62, 65, and 80 indicate high leverage/influence. 3, 6, 62, and 80 can be dropped from the model. 65 can be left in the model since it is not too much higher in leverage than the other points.

**e part e and f)**

```{r echo = FALSE}
#e part e and f)

shapiro.test(comm_model2$residuals)
dwtest(comm_model2)
```

The high p value calculated from the Shapiro-Wilk test, means that we do not have enough evidence to reject the null hypothesis ($H_0$ : the data is normal). This means our assumption of the normality of the data is held true. The Durbin-Watson test indicates that the model errors are correlated ($H_0$: errors are not correlated, $H_A$: They are correlated). The small p value ( < 0.05) means that we have enough evidence to reject the null hypothesis.

### Part F

```{r echo = FALSE}
# Part F
comm_model3 <- lm(rental_rates ~ . - vac_rates, df_commercial[-c(3, 6, 62, 80), ])

summary(comm_model3)
```

By dropping outliers, $R^2$ values did not improve in the new model. They actually dropped slightly, indicating that outliers might not have had a strong effect on the model after all.

```{r echo = FALSE}
comm_model4 <- lm(rental_rates ~ . - vac_rates, df_commercial[-c(6, 62, 80), ])

summary(comm_model4)
```

Leaving data point 3 in the model improved the $R^2$ values a little. 

```{r echo = FALSE}
comm_model5 <- lm(rental_rates ~ . - vac_rates, df_commercial[-c(6, 62), ])

summary(comm_model5)
```

Removing outliers and ignoring high leverage also improved the model slightly as can be seen by the higher $R^2$ values.

### Part G

```{r echo = FALSE}
# G 
single_unit <- tibble(age = 5, opp_expenses = 8.25, vac_rates = 0, tot_squft = 250000)

predict(comm_model5, newdata = single_unit, level = 0.90, interval = "prediction")
```

\newpage

## 5A

```{r echo = FALSE}
df_software <- read_table("software_sales.txt") 
df_software$Software <- as.factor(df_software$Software)

#5A
sw_model1 <- lm(SalesThisQuarter ~ . - SalesLastQuarter - Rep, data = df_software)
summary(sw_model1)
```

### Part I

y = 81.6 - 2.00 $x_1$ - 7.67 $x_2$   
where:    
$x_1$ -> Whether Software 2 was used  
$x_2$ -> Whether Software 3 was used 

$E(y | x_1 = 1, x_2 = 0) = (81.6 - 2.00) - 7.67 * 0 = 79.42$  
$E(y | x_1 = 0, x_2 = 1) = (81.6 - 7.67) - 2.00 * 0 = 73.93$ 


### Part II
```{r echo = FALSE}
# Part ii)
anova(sw_model1)

sprintf("Variance explained by the software is %0.3f", (anova(sw_model1)[1,2] / sum(anova(sw_model1)[,2])))
```

### Part III

$H_0$ states that software has no effect on sales.  
$H_A$ states that software does have an effect on sales.

The *f statistic* given from the anova table, 1.47, and the corresponding *p value*, 0.25, indicate that we do not have enough evidence to reject the null hypothesis at the 0.05 significance level.

## 5B

```{r echo = FALSE}
#5B
sw_model2 <- lm(SalesThisQuarter ~ . -Rep, data = df_software)
summary(sw_model2)
```

### Part I

y = -36.44 + 0.754 $x_1$ - 1.284 $x_2$ + 1.502 $x_3$    
where:    
$x_1$ -> Whether Software 2 was used  
$x_2$ -> Whether Software 3 was used  
$x_3$ -> Sales from last quarter


$E(y | x_1 = 1, x_2 = 0) = (-36.44 + 0.754) - 1.284 * 0 + 1.502 x_3 = -35.69 + 1.502 x_3$  
$E(y | x_1 = 0, x_2 = 1) = (-36.44 - 1.284) + 0.754 * 0 + 1.502 x_3 = -37.72 + 1.502 x_3$


### Part II - V

$H_0$ states that software has no effect on sales.  
$H_A$ states that software does have an effect on sales.

The *f statistic* given from the anova table, 3.76, and the corresponding *p value*, 0.0341, indicates that we do have enough evidence to reject the null hypothesis at the 0.05 significance level.

### Part VI

```{r echo = FALSE}
anova(sw_model2)

sprintf("Variance explained by the software is %0.3f", (anova(sw_model2)[1,2] / sum(anova(sw_model2)[,2])))
```

## 5C

```{r echo = FALSE}
sw_model3 <- lm(SalesThisQuarter ~ SalesLastQuarter:Software + SalesLastQuarter, data = df_software)
summary(sw_model3)
```

### Part I - III
y = -36.22 + 1.504 $x_1$ + 1.504 $x_2$ + 1.482 $x_3$  
where:    
$x_1$ -> Interaction term between sales last quarter and software 1    
$x_2$ -> Interaction term between sales last quarter and software 2  
$x_3$ -> Interaction term between sales last quarter and software 4  

Part i)

$E(y | x_1 = 1, x_2 = 0, x_3 = 0) = (-36.22 + 1.504) - 7.67 * 0 = -34.72$  
$E(y | x_1 = 0, x_2 = 1, x_3 = 0) = (-36.22 + 1.504) - 2.00 * 0 = -34.72$   
$E(y | x_1 = 0, x_2 = 0, x_3 = 1) = (-36.22 + 1.482) - 2.00 * 0 = -34.74$

```{r echo = FALSE}
null_model1 <- lm(SalesThisQuarter ~ NULL, data = df_software)
knitr::kable(anova(null_model1, sw_model1))
knitr::kable(anova(sw_model1))

SST <- sum((df_software$SalesThisQuarter - mean(df_software$SalesThisQuarter))^2)
SSR <- sum((sw_model1$fitted.values - mean(df_software$SalesThisQuarter))^2)
SSE <- sum((df_software$SalesThisQuarter - sw_model1$fitted.values)^2)
MSR <- SSR/(sw_model1$rank - 1)
MSE <- SSE/(nrow(df_software) - sw_model1$rank)
df_vector <- c(sw_model1$rank - 1, nrow(df_software) - sw_model1$rank, nrow(df_software) - 1)
SS <- c(SSR, SSE, SST)

anova_table <- as_tibble(cbind(df_vector, SS))

anova_table <- anova_table %>% mutate(MS = SS/df_vector)

anova_table[3,3] <- NA

F_stat <- c(as.numeric(anova_table[1,3] / anova_table[2,3]), NA, NA)

p_value <- c(1 - (pf(F_stat[1], (sw_model1$rank - 1) , (nrow(df_software) - sw_model1$rank))), NA, NA)

anova_table <- cbind(anova_table, F_stat, p_value)

knitr::kable(anova_table)
```

\newpage

## 6

**part a**

```{r echo = FALSE}
df_pigs <- read_table("pig_weight.txt")
df_pigs$Drug <- as.factor(df_pigs$Drug)
pig_model1 <- lm(Pigweight ~ ., data = df_pigs)
summary(pig_model1)
pig_model1$coefficients[1] + pig_model1$coefficients[2]
pig_model1$coefficients[1] + pig_model1$coefficients[3]
```

$$y = \beta_0 + \beta_1 x_1 +  \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4 + \epsilon$$

$x_1$ -> Whether drug 2 was administered  
$x_2$ -> Whether drug 3 was administered  
$x_3$ -> Mom weight  
$x_4$ -> Dad weight

**part b**

|*Drug*|$E(y | x)$|
|-|-|
|1|7.48 - 0.264 $x_3$ + 0.174 $x_4$|
|2|5.88 - 0.264 $x_3$ + 0.174 $x_4$|
|3|6.78 - 0.264 $x_3$ + 0.174 $x_4$|

**part c**

```{r echo = FALSE}
# Part c
new_pig <- tibble(Drug = as.factor(2), Momweight = 140, Dadweight = 185)

predict(pig_model1, newdata = new_pig, interval = "prediction", level = 0.95)
```

**part d**

```{r echo = FALSE}
# Part d
delta_2_3 <- summary(pig_model1)$coefficient[2,1] - summary(pig_model1)$coefficient[3,1]

SE_delta <- summary(pig_model1)$coefficient[2,2] + summary(pig_model1)$coefficient[3,2]

t_cv <- qt(0.95, 68)

d2d3_CI <- delta_2_3 + c(-1, 1) * SE_delta * t_cv
```

The 95% confidence interval is $$\Delta_{drug 2, drug3} \pm t_{critical value} * SE_{drug2 + drug 3}$$

which calculates to be `r d2d3_CI`.

**part e. i)**

```{r echo = FALSE}
#Part e
pig_model_nodrug <- lm(Pigweight ~ Momweight + Dadweight, data = df_pigs)
pig_model_noparent <- lm(Pigweight ~ Drug, data = df_pigs)

anova(pig_model_nodrug, pig_model1)
#anova(pig_model_noparent, pig_model1)

#anova(pig_model1)
```

$H_0: \beta_0 = \beta_1 = \beta_2 = 0$  
$H_A: \beta_0 \neq 0 \text{ or } \beta_1 \neq 0 \text{ or } \beta_2 \neq 0$ 

In this case, the null hypothesis can be rejected at the 0.05 significance level according to the ANOVA table. The type of drug does have an effect on the final pig weight.

**part e. ii)**

**part e. iii)**

**part e. iv)**

\newpage 


## Code
```{r, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}
```